# -*- coding: utf-8 -*-
## Run on Google Colab
"""Lab 7 - Finetuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19p_1Uz9ZlakdRQnoJfz5Jwfj9xDGFKcA

# Overview
Build a fine-tuned language model that generates ASCII cat art using open source llama model and the Unsloth library. You'll learn how to:

Load and prepare a pre-trained llama model.
Format a custom dataset for supervised fine-tuning.
Apply LoRA adapters for parameter-efficient training.
Fine-tune a model and evaluate results

# Section 1: Install Unsloth

* What is Unsloth?
    A library that makes AI training 2-5x faster and uses 70% less memory.
* This library provides optimized implementations of language models
"""

!pip install unsloth
# Also install supporting packages
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

from unsloth import FastLanguageModel
import torch
from datasets import load_dataset, Dataset
from trl import SFTTrainer
from transformers import TrainingArguments

print("Loading model...")

max_seq_length = 2048  # Can increase to 4096 if needed
dtype = None           # Auto-detect. Use Float16 for older GPUs
load_in_4bit = True    # Use 4-bit quantization to save memory

# Load the Llama 3.2 3B model with 4-bit quantization already applied
# This is a pre-trained base model that hasn't seen ASCII art before
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3.2-3b-bnb-4bit",  # Fast 3B model (fits in Colab free tier!)
    max_seq_length=max_seq_length,
    dtype=dtype,
    # Enable 4-bit quantization to reduce memory usage by ~75%
    # This compresses the model from 32-bit to 4-bit precision
    # Allows 3B parameter model to fit in ~4GB instead of ~12GB
    load_in_4bit=load_in_4bit,
)

tokenizer.pad_token = tokenizer.eos_token
print("‚úÖ Model loaded successfully!")

"""# Section 2: Test base model


"""

print("\nüß™ Testing BASE model (before training)...\n")

# Run the model 5 times to see what it generates WITHOUT fine-tuning
for i in range(5):

  # Put the model in inference mode (turn off training features)
  FastLanguageModel.for_inference(model)

  # Convert the text prompt into token IDs that the model understands
  # Tokens are numbers that represent words/characters
  inputs = tokenizer(
      ["Following is the Ascii cat art"],  # Give it a prompt
      return_tensors="pt"
  ).to("cuda")

  outputs = model.generate(
      **inputs,                               # The tokenized input
       max_new_tokens=150,                    # Generate up to 150 new tokens
      temperature=0.8,                        # Controls randomness (0.0=deterministic, 2.0=very random)
      do_sample=True,                         # Enable random sampling (vs. greedy)
      pad_token_id=tokenizer.eos_token_id,    # What token to use for padding
  )

  # Convert the generated token IDs back into readable text
  base_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
  print("Base model output:")
  print(base_output)
  print("\n" + "=" * 80)

"""# Section 2 Review:


*   What is the output of the base model when prompted to generate ascii cat art?

*The base model, when prompted to generate ASCII cat art, produces ordinary text or gibberish rather than actual ASCII drawings‚Äîdemonstrating that fine-tuning is required to teach it ASCII-style output.*

# Section 3: Load Ascii cat dataset
"""

#Load Ascii cat dataset
dataset = load_dataset("pookie3000/ascii-cats", split="train")

print(f"‚úÖ Dataset loaded with {len(dataset)} ASCII cats!")
print(f"\nüê± Sample ASCII cat from dataset:")

# Display 5 random examples from the dataset
# Each example has an "ascii" field containing the cat art
for i in range(5):
    print(dataset[i]["ascii"])
print("=" * 80)

"""# Section 4: Format dataset for training"""

print("\n‚öôÔ∏è  Formatting dataset...")

# Get the special token that marks the end of a sequence
# This tells the model "stop generating here"
EOS_TOKEN = tokenizer.eos_token

# Create a template for how each training example should look
# The model will learn to generate content in this format
alpaca_prompt = """### ASCII Art:
{ascii_art}"""

def formatting_prompts_func(examples):
    ascii_art_samples = examples["ascii"]
    training_prompts = []

    for ascii_art in ascii_art_samples:
        # Clean the ASCII art (remove extra whitespace)
        ascii_art_clean = ascii_art.strip()

        # Insert the ASCII art into our template and add end token
        # This creates the complete training example
        training_prompt = alpaca_prompt.format(ascii_art=ascii_art_clean) + EOS_TOKEN
        training_prompts.append(training_prompt)

    return {"text": training_prompts}

# Apply the formatting function to all examples in the dataset
# batched=True processes multiple examples at once (faster)
dataset = dataset.map(formatting_prompts_func, batched=True)

print(f"‚úÖ Dataset formatted!")
print(f"\nüìù Sample formatted example:")
print(repr(dataset[0]["text"][:150]))

"""# Section 5: Split Dataset"""

print("\nüìä Splitting dataset...")

# Calculate how many examples to use for training (90% of dataset, max 300)
train_size = min(300, int(0.9 * len(dataset)))

# Calculate validation size (10% of dataset, max 30)
# Validation data is used to check if the model is learning well
eval_size = min(30, len(dataset) - train_size)

train_dataset = dataset.select(range(train_size))
eval_dataset = dataset.select(range(train_size, train_size + eval_size))

print(f"Training: {len(train_dataset)} examples")
print(f"Validation: {len(eval_dataset)} examples")
print("=" * 80)

"""# Section 6: Add LoRA adapters


* Apply LoRA (Low-Rank Adaptation) to the model
* This makes training much faster and uses less memory
* Instead of training all 3 billion parameters, LoRA adds small "adapter" layers



"""

print("\n‚öôÔ∏è  Adding LoRA adapters...")

model = FastLanguageModel.get_peft_model(
    model,
    r=2,  # Rank of the LoRA matrices (higher = more capacity but slower)
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",  # Which parts of the model to adapt. These are the attention and feedforward layers in the transformer
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

# Calculate how many parameters are actually being trained
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())

print(f"‚úÖ LoRA adapters added!")
print(f"üìä Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")
print("=" * 80)

"""# Section 6 Review:

*   What percentage of the model's parameters are trainable after adding LoRA? Write down both the number of trainable parameters and the percentage.
*After adding LoRA (Low-Rank Adaptation) adapters to the model, only a very small subset of the parameters are trainable: Trainable parameters: 3,039,232 and Total parameters: 1,806,502,912 i.e., Percentage trainable: ‚âà 0.17 %*

# Section 6: Configure Training
"""

print("\n‚öôÔ∏è  Configuring trainer...")

trainer = SFTTrainer(
    model=model,                                    # The model with LoRA adapters
    tokenizer=tokenizer,                            # The tokenizer for text processing
    train_dataset=train_dataset,                    # Training data
    eval_dataset=eval_dataset,                      # Validation data
    dataset_text_field="text",                      # Which field contains the text
    max_seq_length=2048,                            # Max sequence length for processing
    dataset_num_proc=2,                             # Number of processes to use for dataset processing
    packing=False,                                  # Don't pack multiple examples together
    args=TrainingArguments(
        per_device_train_batch_size=2,              # Process 2 examples at a time per GPU
        gradient_accumulation_steps=4,
        warmup_steps=5,                             # Gradually increase learning rate for first 5 steps
        num_train_epochs=2,                         # Increase epochs for better results # Taking 2 because over 3, responses are overfitting
        learning_rate=2e-4,                         # Higher learning rate for better results
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,                           # Print training stats every 10 steps
        optim="adamw_8bit",
        weight_decay=0.01,                          # Regularization to prevent overfitting
        lr_scheduler_type="linear",                 # Gradually decrease learning rate
        seed=3407,                                  # Random seed for reproducibility
        output_dir="outputs",                       # Where to save checkpoints
        report_to="none",                           # Don't send metrics to external services
    ),
)

print("‚úÖ Trainer configured!")

"""# Section 7: Train!"""

print("\nüöÄ Starting training...")
print("=" * 80)

# Get GPU information to monitor memory usage
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)

print(f"GPU: {gpu_stats.name}")
print(f"Max memory: {max_memory} GB")
print(f"Used before training: {start_gpu_memory} GB\n")

# Run the training process!
# This is where the model learns to generate ASCII cats
# It will take 5-15 minutes depending on your GPU
trainer_stats = trainer.train()

print("\n" + "=" * 80)
print("‚úÖ TRAINING COMPLETE!")
print("=" * 80)

used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
print(f"Peak memory: {used_memory} GB")
print(f"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds")
print(f"Training loss: {trainer_stats.metrics['train_loss']:.4f}")

FastLanguageModel.for_inference(model)
print("‚úÖ Inference mode enabled!")

"""# Section 7 Review

*   What do you notice in the training loss data in the output. Do you notice a pattern, what does that indicate?
Ans. The training loss consistently decreases as the training progresses ‚Äî from around 3.93 to 1.89 by step 60.
This downward trend indicates that the model is learning effectively ‚Äî it‚Äôs gradually improving its ability to predict the next tokens based on the training data (in this case, ASCII cat art patterns).
A steadily declining loss means the model is converging ‚Äî adapting its weights (especially the LoRA adapters) to minimize prediction error.
The final average loss (‚âà 2.63) is significantly lower than the initial loss (~3.9), suggesting a successful fine-tuning run.
If the loss had fluctuated wildly or increased, that would signal instability or overfitting ‚Äî but here, the smooth decline shows stable learning.
**In summary, the training loss decreases steadily over time, showing that the model is successfully learning the ASCII art generation task and converging as training progresses.**

"""



"""# Section 8: Test the fine-tuned model"""

# Generate 10 ASCII cats using the newly trained model
for i in range(10):  # Generate 10 cats
    print(f"\nüê± Cat #{i+1}:")
    print("-" * 80)

    # Convert prompt to tokens
    inputs = tokenizer(
        ["### Following are ASCII cat art.\n"],
        return_tensors="pt"
    ).to("cuda")

    # Generate with fine-tuned parameters
    outputs = model.generate(
        **inputs,
        max_new_tokens=400,
        min_new_tokens=10,
        temperature=0.8,  # Balanced
        top_p=0.85,
        top_k=40,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        repetition_penalty=1.05,
        no_repeat_ngram_size=0,
    )


    # Decode the generated tokens back to text
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

     # Extract ASCII art
    if "### ASCII Cat:" in result:
        ascii_art = result.split("### ASCII Cat:")[1].strip()
        print(ascii_art)
    else:
        print(result)

"""Section 8 Review:

*   What difference do you see between base model output in section 2 and trained model in section 8?
*Ans. The fine-tuned model learned the formatting and spatial structure of ASCII cats, transforming its text output into coherent symbolic art, something completely absent in the base model.*
*Increasing LoRA rank and epochs allows the fine-tuned model to store richer representations of ASCII-art structure, yielding cleaner, more varied, and more visually appealing cats‚Äîdemonstrating the clear effect of higher model capacity and longer training on output quality.*
*   Increase the number of trainable parameters in lora adapter "r" and increase the number of epochs in section 6. You will have to rerun all the cells from the start. What difference do you notice in the output compared to the previous one.
*   Tip: keep both the values below 10 to save time

"""