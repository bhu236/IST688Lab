{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhu236/IST688Lab/blob/main/Lab9%3AAI_Memory_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246a8966",
      "metadata": {
        "id": "246a8966"
      },
      "source": [
        "\n",
        "# **Assignment: Long-Term Memory in AI — From RAG to Advanced Architectures**\n",
        "\n",
        "**Objective:** This assignment will help you understand the concepts of memory in detail.\n",
        "\n",
        "- Uses a tiny **SQuAD-style** embedded dataset (famous dataset family, small educational subset here to keep it offline).\n",
        "\n",
        "> Complete code where marked **TODO**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "349aed80",
      "metadata": {
        "id": "349aed80"
      },
      "source": [
        "\n",
        "##  Setup & Mini SQuAD-Style Dataset\n",
        "\n",
        "We embed a **small educational subset** inspired by the famous **SQuAD** question-answering dataset.  \n",
        "Each record includes a short *context* paragraph, a *question*, and a *reference answer*.\n",
        "\n",
        "We'll use this as our knowledge base for RAG and downstream memory demos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c607f61d",
      "metadata": {
        "id": "c607f61d",
        "outputId": "a071f3db-9b6c-4c01-85f2-2fa946c01d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded mini SQuAD-style dataset with 5 examples.\n",
            "{'answer': 'building real-time data pipelines and streaming apps',\n",
            " 'context': 'Kafka is a distributed streaming platform used for building '\n",
            "            'real-time data pipelines and streaming apps. It is horizontally '\n",
            "            'scalable and fault-tolerant.',\n",
            " 'id': '1',\n",
            " 'question': 'What is Kafka used for?'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Standard libraries only; no internet access is required.\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "# Tiny, self-contained SQuAD-style subset for class practice (educational use).\n",
        "# Each item: {\"id\": str, \"context\": str, \"question\": str, \"answer\": str}\n",
        "SQUAD_MINI = [\n",
        "    {\n",
        "        \"id\": \"1\",\n",
        "        \"context\": \"Kafka is a distributed streaming platform used for building real-time data pipelines and streaming apps. It is horizontally scalable and fault-tolerant.\",\n",
        "        \"question\": \"What is Kafka used for?\",\n",
        "        \"answer\": \"building real-time data pipelines and streaming apps\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"2\",\n",
        "        \"context\": \"Prometheus is an open-source monitoring system with a dimensional data model, flexible query language, and built-in alerting.\",\n",
        "        \"question\": \"What kind of system is Prometheus?\",\n",
        "        \"answer\": \"an open-source monitoring system\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"3\",\n",
        "        \"context\": \"R-trees are tree data structures used for spatial access methods, allowing for efficient querying of geographical data like rectangles or polygons.\",\n",
        "        \"question\": \"What are R-trees used for?\",\n",
        "        \"answer\": \"spatial access methods and efficient querying of geographical data\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"4\",\n",
        "        \"context\": \"Elastic Weight Consolidation is a technique in continual learning that reduces catastrophic forgetting by selectively slowing down learning on weights important to previous tasks.\",\n",
        "        \"question\": \"What problem does Elastic Weight Consolidation address?\",\n",
        "        \"answer\": \"catastrophic forgetting\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"5\",\n",
        "        \"context\": \"A knowledge graph represents entities and their relationships, enabling structured reasoning and linking between facts.\",\n",
        "        \"question\": \"What does a knowledge graph represent?\",\n",
        "        \"answer\": \"entities and their relationships\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Loaded mini SQuAD-style dataset with\", len(SQUAD_MINI), \"examples.\")\n",
        "pprint(SQUAD_MINI[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "453ba781",
      "metadata": {
        "id": "453ba781"
      },
      "source": [
        "\n",
        "## 1) RAG (Retrieval-Augmented Generation) with TF-IDF (Foundational)\n",
        "\n",
        "In a typical RAG pipeline:\n",
        "1. **Embed or featurize** documents (here: TF-IDF for simplicity).\n",
        "2. **Retrieve** the most relevant chunks for a user query.\n",
        "3. **Augment** the model prompt/logic with those top chunks to produce an answer.\n",
        "\n",
        "We'll implement a *minimal* RAG retriever using `sklearn`'s `TfidfVectorizer` (works offline).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aaa9b8c0",
      "metadata": {
        "id": "aaa9b8c0",
        "outputId": "66bc6295-de09-418f-a66f-42005f975a22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': '4',\n",
              "  'context': 'Elastic Weight Consolidation is a technique in continual learning that reduces catastrophic forgetting by selectively slowing down learning on weights important to previous tasks.',\n",
              "  'score': 0.33333333333333337},\n",
              " {'id': '5',\n",
              "  'context': 'A knowledge graph represents entities and their relationships, enabling structured reasoning and linking between facts.',\n",
              "  'score': 0.0}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Build a small corpus from the contexts.\n",
        "corpus = [x[\"context\"] for x in SQUAD_MINI]\n",
        "ids = [x[\"id\"] for x in SQUAD_MINI]\n",
        "\n",
        "# Vectorize with TF-IDF (no internet or large models needed).\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(corpus)  # shape: (n_docs, n_terms)\n",
        "\n",
        "def retrieve_contexts(query: str, k: int = 2) -> List[Dict]:\n",
        "    \"\"\"Return top-k contexts with cosine similarity scores.\"\"\"\n",
        "    q_vec = vectorizer.transform([query])\n",
        "    sims = cosine_similarity(q_vec, X).ravel()  # similarity per document\n",
        "    topk_idx = np.argsort(sims)[::-1][:k]\n",
        "    results = []\n",
        "    for idx in topk_idx:\n",
        "        results.append({\n",
        "            \"id\": ids[idx],\n",
        "            \"context\": corpus[idx],\n",
        "            \"score\": float(sims[idx])\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Demo retrieval\n",
        "demo = retrieve_contexts(\"How do I prevent catastrophic forgetting?\", k=2)\n",
        "demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db95f67",
      "metadata": {
        "id": "2db95f67"
      },
      "source": [
        "\n",
        "### **Exercise 1 (RAG):**\n",
        "- **TODO:** Implement a function `rag_answer(query, k=2)` that:\n",
        "  1) calls `retrieve_contexts(query, k)`,\n",
        "  2) **builds a compact answer** by extracting the *most relevant phrase* (you can do simple heuristics like grabbing the sentence with highest overlap),\n",
        "  3) returns both the answer string and the top contexts used.\n",
        "- **Stretch:** Add a simple *citation* mechanism: include the `id` of the top-ranked context next to your answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0c4bedf5",
      "metadata": {
        "id": "0c4bedf5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== YOUR TURN ==========\n",
        "# TODO: Write rag_answer(query, k=2) below.\n",
        "# Hints:\n",
        "# - You can split context into sentences using simple `.split('.')`.\n",
        "# - Score sentences by token overlap with the query (very simple, but OK for now).\n",
        "# - Return {\"answer\": str, \"used_contexts\": List[Dict]}\n",
        "def rag_answer(query: str, k: int = 2):\n",
        "    raise NotImplementedError(\"Implement me\")  # TODO\n",
        "\n",
        "# Test (uncomment when implemented)\n",
        "# print(rag_answer(\"What prevents forgetting in continual learning?\", k=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88fbb288",
      "metadata": {
        "id": "88fbb288"
      },
      "source": [
        "\n",
        "## 2) Episodic Interaction Memory (Persistent Across Sessions)\n",
        "\n",
        "We'll implement a minimal **episodic memory store** that keeps **user-specific** interactions.\n",
        "- `remember_episode(user_id, text, topic)` writes to memory.\n",
        "- `recall_episode(user_id, topic, k)` retrieves recent episodes relevant to a topic.\n",
        "\n",
        "This lets an assistant *carry context forward* across sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aac9a17a",
      "metadata": {
        "id": "aac9a17a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ae7b13-5315-47f5-c96b-638eea0928ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3223773476.py:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"ts\": datetime.utcnow(),\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'ts': datetime.datetime(2025, 11, 13, 16, 6, 12, 56524),\n",
              "  'topic': 'observability',\n",
              "  'text': 'Validated metrics in Prometheus'},\n",
              " {'ts': datetime.datetime(2025, 11, 13, 16, 6, 12, 56254),\n",
              "  'topic': 'observability',\n",
              "  'text': 'Configured JMX for Kafka brokers'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "from collections import defaultdict, deque\n",
        "from datetime import datetime\n",
        "\n",
        "EPISODES = defaultdict(lambda: deque(maxlen=500))  # user_id -> deque of episodes\n",
        "\n",
        "def remember_episode(user_id: str, text: str, topic: str):\n",
        "    EPISODES[user_id].append({\n",
        "        \"ts\": datetime.utcnow(),\n",
        "        \"topic\": topic.lower(),\n",
        "        \"text\": text\n",
        "    })\n",
        "\n",
        "def recall_episode(user_id: str, topic: str, k: int = 3):\n",
        "    topic = topic.lower()\n",
        "    matches = [e for e in reversed(EPISODES[user_id]) if e[\"topic\"] == topic]\n",
        "    if len(matches) < k:  # backfill with recent mixed topics\n",
        "        backfill = [e for e in reversed(EPISODES[user_id]) if e not in matches]\n",
        "        matches.extend(backfill)\n",
        "    return matches[:k]\n",
        "\n",
        "# Demo writes\n",
        "remember_episode(\"u1\", \"Configured JMX for Kafka brokers\", \"observability\")\n",
        "remember_episode(\"u1\", \"Adjusted batch.size and linger.ms\", \"kafka-tuning\")\n",
        "remember_episode(\"u1\", \"Validated metrics in Prometheus\", \"observability\")\n",
        "\n",
        "recall_episode(\"u1\", \"observability\", k=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cf2e1c",
      "metadata": {
        "id": "78cf2e1c"
      },
      "source": [
        "\n",
        "### **Exercise 2 (Episodic):**\n",
        "- **TODO:** Create a helper `episodic_rag(user_id, query)` that:\n",
        "  1) retrieves contexts via **RAG**, and\n",
        "  2) **prepends** the top episodic memory snippet (if topic overlaps with the query).\n",
        "- **Tip:** You may define topic as the **top 1 retrieved doc's id** or based on simple keyword rules.\n",
        "- **Goal:** Show how persistent episodes can *steer* retrieval or answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "faecb128",
      "metadata": {
        "id": "faecb128"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== YOUR TURN ==========\n",
        "# TODO: Implement episodic_rag(user_id, query) returning a dict:\n",
        "# {\"answer\": str, \"episodic_used\": str|None, \"contexts\": List[Dict]}\n",
        "def episodic_rag(user_id: str, query: str):\n",
        "    raise NotImplementedError(\"Implement me\")  # TODO\n",
        "\n",
        "# Test (after implementing)\n",
        "# episodic_rag(\"u1\", \"How to expose Kafka metrics?\" )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0049e50a",
      "metadata": {
        "id": "0049e50a"
      },
      "source": [
        "\n",
        "## 3) Memory-Augmented Reasoning (One Differentiable Read/Write)\n",
        "\n",
        "We simulate a minimal **external memory** accessed by a neural model step.\n",
        "- Content-based addressing (cosine attention) to **read**.\n",
        "- Soft, differentiable **write** to attended slots.\n",
        "\n",
        "> This is **not** full training code — just the memory operation primitive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e59ca51c",
      "metadata": {
        "id": "e59ca51c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdcdee4-7e8c-4481-93f1-c5a48cd7b803"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32]), False)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def memory_step(M: torch.Tensor, key: torch.Tensor, write_val: torch.Tensor, alpha: float = 0.2):\n",
        "    \"\"\"\n",
        "    M: [N, D]  memory matrix\n",
        "    key: [D]   query key\n",
        "    write_val: [D] value to write\n",
        "    returns: read_vec [D], updated memory [N, D]\n",
        "    \"\"\"\n",
        "    k = F.normalize(key, dim=0)\n",
        "    M_norm = F.normalize(M, dim=1)\n",
        "    attn = F.softmax((M_norm @ k), dim=0)  # [N]\n",
        "    read_vec = attn @ M                    # [D]\n",
        "    write = attn.unsqueeze(1) * write_val.unsqueeze(0)  # [N, D]\n",
        "    M_new = M * (1 - alpha * attn.unsqueeze(1)) + alpha * write\n",
        "    return read_vec, M_new\n",
        "\n",
        "# Demo\n",
        "torch.manual_seed(0)\n",
        "M = torch.randn(8, 32)\n",
        "key = torch.randn(32)\n",
        "val = torch.randn(32)\n",
        "r, M2 = memory_step(M, key, val)\n",
        "r.shape, torch.allclose(M, M2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e746064d",
      "metadata": {
        "id": "e746064d"
      },
      "source": [
        "\n",
        "### **Exercise 3 (Memory Step):**\n",
        "- **TODO:** Add **erase** capability:\n",
        "  - Implement an `erase_val` vector (shape `[D]`) and an `erase_rate` in `memory_step` that *reduces* memory content at attended locations before writing.\n",
        "- **Question:** What happens if erase is too strong? Explain the trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f34621e7",
      "metadata": {
        "id": "f34621e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== YOUR TURN ==========\n",
        "# TODO: Re-implement memory_step with erase gate; name it memory_step_erase(...)\n",
        "def memory_step_erase(M: torch.Tensor, key: torch.Tensor, write_val: torch.Tensor, erase_val: torch.Tensor, alpha: float = 0.2, erase_rate: float = 0.2):\n",
        "    raise NotImplementedError(\"Implement erase-enhanced memory step\")  # TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb5fd0f",
      "metadata": {
        "id": "deb5fd0f"
      },
      "source": [
        "\n",
        "## 4) Coordination via Memory (Blackboard Pattern)\n",
        "\n",
        "Agents coordinate by **reading and writing** to a shared blackboard:\n",
        "- `planner` posts steps.\n",
        "- `executor` completes steps and updates status.\n",
        "- `validator` checks outputs.\n",
        "\n",
        "We simulate a tiny asynchronous loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "42755f37",
      "metadata": {
        "id": "42755f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee7d82b-dacf-4272-b049-a613dd2bb920"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True,\n",
              " {'steps': [{'step': 'collect-metrics', 'by': 'planner', 'done': True},\n",
              "   {'step': 'verify-alerts', 'by': 'planner', 'done': True}],\n",
              "  'status': {'collect-metrics': 'completed by executor',\n",
              "   'verify-alerts': 'completed by executor'}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\n",
        "BLACKBOARD = {\"steps\": [], \"status\": {}}\n",
        "\n",
        "def post_step(step: str, by: str):\n",
        "    BLACKBOARD[\"steps\"].append({\"step\": step, \"by\": by, \"done\": False})\n",
        "\n",
        "def execute_next(by: str):\n",
        "    for item in BLACKBOARD[\"steps\"]:\n",
        "        if not item[\"done\"]:\n",
        "            # simulate work\n",
        "            item[\"done\"] = True\n",
        "            BLACKBOARD[\"status\"][item[\"step\"]] = f\"completed by {by}\"\n",
        "            return item[\"step\"]\n",
        "    return None\n",
        "\n",
        "def validate_all(by: str):\n",
        "    return all(s.endswith(\"completed by executor\") for s in BLACKBOARD[\"status\"].values())\n",
        "\n",
        "# Demo\n",
        "post_step(\"collect-metrics\", \"planner\")\n",
        "post_step(\"verify-alerts\", \"planner\")\n",
        "execute_next(\"executor\")\n",
        "execute_next(\"executor\")\n",
        "validate_all(\"validator\"), BLACKBOARD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a930d12",
      "metadata": {
        "id": "6a930d12"
      },
      "source": [
        "\n",
        "### **Exercise 4 (Coordination):**\n",
        "- **TODO:** Add a **moderation layer** that validates step names against a whitelist before posting (e.g., only `collect-metrics`, `verify-alerts` allowed).\n",
        "- **Stretch:** Add a **priority** field so the executor picks highest priority first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "814dbea9",
      "metadata": {
        "id": "814dbea9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== YOUR TURN ==========\n",
        "ALLOWED_STEPS = {\"collect-metrics\", \"verify-alerts\"}\n",
        "\n",
        "# TODO: Wrap post_step with moderation: post_step_safe(step, by) -> bool (accepted?)\n",
        "def post_step_safe(step: str, by: str) -> bool:\n",
        "    raise NotImplementedError(\"Implement moderation and optional priority logic\")  # TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efef7f18",
      "metadata": {
        "id": "efef7f18"
      },
      "source": [
        "\n",
        "## 5) Federated Memory (Edge Summaries → Central Aggregation)\n",
        "\n",
        "Each edge node keeps **local memory** and periodically shares **summaries** (not raw data) with a central aggregator.\n",
        "We simulate:\n",
        "- Per-node **local_stats**\n",
        "- Summaries with **differential privacy noise** (simple Laplace) to protect details\n",
        "- Central **aggregate** that fuses these summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f69dbf33",
      "metadata": {
        "id": "f69dbf33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45bf4362-d75e-4884-a119-afa37358b7f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'global_success_rate': 0.9252623517160539, 'n': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "def laplace_noise(scale: float = 1.0):\n",
        "    # Simple Laplace noise for DP-like behavior (educational only)\n",
        "    u = rng.uniform(-0.5, 0.5)\n",
        "    return -scale * np.sign(u) * np.log(1 - 2 * abs(u))\n",
        "\n",
        "class EdgeNode:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.local_stats = {\"queries\": 0, \"success\": 0}\n",
        "    def observe(self, ok: bool):\n",
        "        self.local_stats[\"queries\"] += 1\n",
        "        self.local_stats[\"success\"] += int(ok)\n",
        "    def summarize(self, noise_scale: float = 0.5):\n",
        "        # share noisy rate only\n",
        "        q = max(1, self.local_stats[\"queries\"])  # avoid div by zero\n",
        "        rate = self.local_stats[\"success\"]/q + laplace_noise(noise_scale)\n",
        "        return {\"node\": self.name, \"success_rate\": float(rate)}\n",
        "\n",
        "def central_aggregate(summaries: List[Dict]):\n",
        "    # clamp and average\n",
        "    rates = [min(1.0, max(0.0, s[\"success_rate\"])) for s in summaries]\n",
        "    return {\"global_success_rate\": float(np.mean(rates)), \"n\": len(rates)}\n",
        "\n",
        "# Demo\n",
        "e1, e2 = EdgeNode(\"edge-a\"), EdgeNode(\"edge-b\")\n",
        "for _ in range(10): e1.observe(ok=True)\n",
        "for _ in range(10): e2.observe(ok=(rng.random() > 0.3))\n",
        "summ = [e1.summarize(), e2.summarize()]\n",
        "central_aggregate(summ)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00292d5c",
      "metadata": {
        "id": "00292d5c"
      },
      "source": [
        "\n",
        "### **Exercise 5 (Federated):**\n",
        "- **TODO:** Extend `central_aggregate` to also compute a **weighted** global rate using the **true query counts** privately shared as integers (no noise on counts).  \n",
        "- **Discuss:** Trade-offs between noisy metrics vs. exact counts in privacy-preserving systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5e9f351d",
      "metadata": {
        "id": "5e9f351d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== YOUR TURN ==========\n",
        "# TODO: Modify summarize() to also return raw count, and central_aggregate() to compute weighted rate.\n",
        "# You may create new functions summarize_with_count() and central_aggregate_weighted(...)\n",
        "def summarize_with_count(node: EdgeNode, noise_scale: float = 0.5):\n",
        "    raise NotImplementedError(\"Return dict with noisy rate and raw count\")  # TODO\n",
        "\n",
        "def central_aggregate_weighted(summaries: List[Dict]):\n",
        "    raise NotImplementedError(\"Compute weighted success rate using counts\")  # TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4e5b35",
      "metadata": {
        "id": "fc4e5b35"
      },
      "source": [
        "\n",
        "## 6) Putting It Together (Mini Workflow)\n",
        "\n",
        "We combine:\n",
        "- RAG retrieval\n",
        "- Episodic prime\n",
        "- Consolidation events\n",
        "- Telemetry\n",
        "\n",
        "Run this end-to-end to see interactions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef23f6b5",
      "metadata": {
        "id": "ef23f6b5"
      },
      "source": [
        "\n",
        "### **Exercise 6 (Integration):**\n",
        "- **TODO:** Modify `mini_assistant` to:\n",
        "  - Use your `rag_answer` from Exercise 1 instead of returning raw context.\n",
        "  - Include a citation (top context `id`) in the final string.\n",
        "  - Log a simple **trace record** that lists which components were used (episodic, rag, consolidation).\n",
        "- **Reflection:** What additional safeguards or governance would you add for a production system?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7cff6bd2",
      "metadata": {
        "id": "7cff6bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "89d40eeb-26e7-45bf-95b1-da154dbe4f9d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'put_item' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2169162217.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmini_assistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"u1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"How do I expose Kafka metrics to Prometheus?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2169162217.py\u001b[0m in \u001b[0;36mmini_assistant\u001b[0;34m(user_id, query)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 4) consolidate a synthetic key for demonstration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mput_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last_query_topic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"contexts\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last_query_topic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconsolidate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'put_item' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "def mini_assistant(user_id: str, query: str):\n",
        "    # 1) episodic prime\n",
        "    episodes = recall_episode(user_id, topic=\"observability\", k=1)  # as an example\n",
        "    epi_text = episodes[0][\"text\"] if episodes else None\n",
        "\n",
        "    # 2) RAG\n",
        "    contexts = retrieve_contexts(query, k=2)\n",
        "\n",
        "    # 3) naive answer (just return top context for demo)\n",
        "    answer = contexts[0][\"context\"] if contexts else \"No context found.\"\n",
        "\n",
        "    # 4) consolidate a synthetic key for demonstration\n",
        "    put_item(\"last_query_topic\", {\"query\": query, \"contexts\": [c[\"id\"] for c in contexts]})\n",
        "    _ = get_item(\"last_query_topic\")\n",
        "    consolidate_memory()\n",
        "\n",
        "    return {\"episodic\": epi_text, \"answer\": answer, \"contexts\": contexts}\n",
        "\n",
        "# Demo\n",
        "mini_assistant(\"u1\", \"How do I expose Kafka metrics to Prometheus?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dmyCCDGKkUCd"
      },
      "id": "dmyCCDGKkUCd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 7 (Review questions):**"
      ],
      "metadata": {
        "id": "g11ogPiOkUOH"
      },
      "id": "g11ogPiOkUOH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\tHow does pruning affect personalization?\n",
        "\n",
        "•\tWhen should user consent matter most for stored facts? Why?\n",
        "\n",
        "•\tWhat are the risks if an AI agent never forgets anything?\n"
      ],
      "metadata": {
        "id": "ypPbaOrPkgi0"
      },
      "id": "ypPbaOrPkgi0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Write your answers here_"
      ],
      "metadata": {
        "id": "g9mayJjGkkB8"
      },
      "id": "g9mayJjGkkB8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}